{"block_file": {"custom/dbt_runner.py:custom:python:dbt runner": {"content": "import os\nimport subprocess\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n@custom\ndef run_dbt(*args, **kwargs):\n    \"\"\"\n    Execute dbt run directly with environment variables\n    \"\"\"\n    # Set environment variables from Mage secrets\n    from mage_ai.data_preparation.shared.secrets import get_secret_value\n    \n    env = os.environ.copy()\n    env['SNOWFLAKE_ACCOUNT'] = get_secret_value('SNOWFLAKE_ACCOUNT')\n    env['SNOWFLAKE_USER'] = get_secret_value('SNOWFLAKE_USER')\n    env['SNOWFLAKE_PASSWORD'] = get_secret_value('SNOWFLAKE_PASSWORD')\n    env['SNOWFLAKE_DATABASE'] = get_secret_value('SNOWFLAKE_DATABASE')\n    env['SNOWFLAKE_WAREHOUSE'] = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    env['SNOWFLAKE_ROLE'] = get_secret_value('SNOWFLAKE_ROLE')\n    \n    # Run dbt\n    result = subprocess.run(\n        ['dbt', 'run', '--profiles-dir', '/home/src', '--project-dir', '/home/src'],\n        env=env,\n        capture_output=True,\n        text=True\n    )\n    \n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n    \n    if result.returncode != 0:\n        raise Exception(f\"dbt run failed with code {result.returncode}\")\n    \n    return {\"status\": \"success\", \"output\": result.stdout}\n", "file_path": "custom/dbt_runner.py", "language": "python", "type": "custom", "uuid": "dbt_runner"}, "custom/dbt_test.py:custom:python:dbt test": {"content": "import os\nimport subprocess\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n@custom\ndef run_dbt_tests(*args, **kwargs):\n    \"\"\"\n    Execute dbt test\n    \"\"\"\n    from mage_ai.data_preparation.shared.secrets import get_secret_value\n\n    env = os.environ.copy()\n    env['SNOWFLAKE_ACCOUNT'] = get_secret_value('SNOWFLAKE_ACCOUNT')\n    env['SNOWFLAKE_USER'] = get_secret_value('SNOWFLAKE_USER')\n    env['SNOWFLAKE_PASSWORD'] = get_secret_value('SNOWFLAKE_PASSWORD')\n    env['SNOWFLAKE_DATABASE'] = get_secret_value('SNOWFLAKE_DATABASE')\n    env['SNOWFLAKE_WAREHOUSE'] = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    env['SNOWFLAKE_ROLE'] = get_secret_value('SNOWFLAKE_ROLE')\n\n    result = subprocess.run(\n        ['dbt', 'test', '--profiles-dir', '/home/src', '--project-dir', '/home/src'],\n        env=env,\n        capture_output=True,\n        text=True\n    )\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise Exception(f\"dbt test failed with code {result.returncode}\")\n\n    return {\"status\": \"success\", \"output\": result.stdout}", "file_path": "custom/dbt_test.py", "language": "python", "type": "custom", "uuid": "dbt_test"}, "custom/dbt_docs.py:custom:python:dbt docs": {"content": "import os\nimport subprocess\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n@custom\ndef generate_dbt_docs(*args, **kwargs):\n    from mage_ai.data_preparation.shared.secrets import get_secret_value\n\n    env = os.environ.copy()\n    env['SNOWFLAKE_ACCOUNT'] = get_secret_value('SNOWFLAKE_ACCOUNT')\n    env['SNOWFLAKE_USER'] = get_secret_value('SNOWFLAKE_USER')\n    env['SNOWFLAKE_PASSWORD'] = get_secret_value('SNOWFLAKE_PASSWORD')\n    env['SNOWFLAKE_DATABASE'] = get_secret_value('SNOWFLAKE_DATABASE')\n    env['SNOWFLAKE_WAREHOUSE'] = get_secret_value('SNOWFLAKE_WAREHOUSE')\n    env['SNOWFLAKE_ROLE'] = get_secret_value('SNOWFLAKE_ROLE')\n\n    result = subprocess.run(\n        ['dbt', 'docs', 'generate', '--profiles-dir', '/home/src', '--project-dir', '/home/src'],\n        env=env,\n        capture_output=True,\n        text=True\n    )\n\n    print(result.stdout)\n    if result.stderr:\n        print(\"STDERR:\", result.stderr)\n\n    if result.returncode != 0:\n        raise Exception(f\"dbt docs generate failed with code {result.returncode}\")\n\n    return {\"status\": \"success\", \"output\": result.stdout}", "file_path": "custom/dbt_docs.py", "language": "python", "type": "custom", "uuid": "dbt_docs"}, "data_exporters/exporter_taxi.py:data_exporter:python:exporter taxi": {"content": "from mage_ai.io.snowflake import Snowflake\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(data: pd.DataFrame, **kwargs) -> None:\n\n    # Crear conexi\u00f3n directa a Snowflake\n    loader = Snowflake(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema='RAW',\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        insecure_mode=True,\n    )\n\n    table_name = \"taxi_zones\"\n\n    with loader:\n        # exportar\n        loader.export(\n            data,\n            table_name,\n            if_exists='replace'\n        )\n\n    print(f\"Exportaci\u00f3n finalizada\")\n", "file_path": "data_exporters/exporter_taxi.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxi"}, "data_exporters/taxi_data_exporter.py:data_exporter:python:taxi data exporter": {"content": "from mage_ai.io.snowflake import Snowflake\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport hashlib\nimport psutil\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(data: pd.DataFrame, **kwargs) -> None:\n    loader = Snowflake(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema='RAW',\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        insecure_mode=True,\n    )\n\n    for idx, row in data.iterrows():\n        if row[\"status\"] != \"ok\":\n            print(f\"Skipping {row['run_id']} - status: {row['status']}\")\n            continue\n\n        run_id = row[\"run_id\"]\n        servicio = row[\"servicio\"]\n        anio = row[\"anio\"]\n        mes = row[\"mes\"]\n\n        local_file = f\"/tmp/{servicio}_{anio}_{mes:02d}.parquet\"\n        if not os.path.exists(local_file):\n            raise FileNotFoundError(f\"File not found: {local_file}\")\n\n        table_name = f\"taxi_{servicio}_data\"\n        parquet_file = pq.ParquetFile(local_file)\n\n        available_mem = psutil.virtual_memory().available\n        approx_row_mem = 600\n        max_batch_from_mem = int((available_mem * 0.80) / approx_row_mem)\n        batch_size = min(3_000_000, max(100_000, max_batch_from_mem))\n\n        print(f\"Processing {run_id} - RAM: {available_mem / (1024**3):.2f}GB - Batch: {batch_size:,} rows\")\n\n        with loader:\n            for batch_idx, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n                df = batch.to_pandas()\n\n                for k, v in row.items():\n                    if k not in df.columns:\n                        df[k] = v\n\n                hash_input = (\n                    df[\"VendorID\"].astype(str) + \"_\" +\n                    df[\"tpep_pickup_datetime\"].astype(str) + \"_\" +\n                    df[\"tpep_dropoff_datetime\"].astype(str)\n                )\n                df[\"row_hash\"] = [hashlib.md5(x.encode()).hexdigest() for x in hash_input.values]\n\n                df.columns = [c.upper() for c in df.columns]\n\n                loader.export(\n                    df,\n                    table_name,\n                    if_exists='append',\n                )\n\n                print(f\"  Batch {batch_idx} exported: {len(df):,} rows\")\n\n                del df, batch, hash_input\n                gc.collect()\n\n        print(f\"Completed {run_id}\")\n        os.remove(local_file)\n", "file_path": "data_exporters/taxi_data_exporter.py", "language": "python", "type": "data_exporter", "uuid": "taxi_data_exporter"}, "data_exporters/green_exporter.py:data_exporter:python:green exporter": {"content": "from mage_ai.io.snowflake import Snowflake\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport hashlib\nimport psutil\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(data: pd.DataFrame, **kwargs) -> None:\n    loader = Snowflake(\n        account=get_secret_value('SNOWFLAKE_ACCOUNT'),\n        user=get_secret_value('SNOWFLAKE_USER'),\n        password=get_secret_value('SNOWFLAKE_PASSWORD'),\n        warehouse=get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        database=get_secret_value('SNOWFLAKE_DATABASE'),\n        schema='RAW',\n        role=get_secret_value('SNOWFLAKE_ROLE'),\n        insecure_mode=True,\n    )\n\n    print(\"Checking existing GREEN RUN_IDs in Snowflake...\")\n    with loader:\n        try:\n            existing_runs_query = \"SELECT DISTINCT RUN_ID FROM RAW.TAXI_GREEN_DATA\"\n            existing_runs_df = loader.load(existing_runs_query)\n            existing_run_ids = set(existing_runs_df['RUN_ID'].tolist()) if not existing_runs_df.empty else set()\n            print(f\"Found {len(existing_run_ids)} existing GREEN RUN_IDs\")\n        except Exception as e:\n            print(f\"Could not verify existing RUN_IDs: {e}\")\n            existing_run_ids = set()\n\n    for idx, row in data.iterrows():\n        if row[\"status\"] != \"ok\":\n            print(f\"Skipping {row['run_id']} - status: {row['status']}\")\n            continue\n\n        run_id = row[\"run_id\"]\n\n        if run_id in existing_run_ids:\n            print(f\"Skipping {run_id} - already processed\")\n            continue\n\n        servicio = row[\"servicio\"]\n        anio = row[\"anio\"]\n        mes = row[\"mes\"]\n\n        local_file = f\"/tmp/{servicio}_{anio}_{mes:02d}.parquet\"\n        if not os.path.exists(local_file):\n            raise FileNotFoundError(f\"File not found: {local_file}\")\n\n        table_name = f\"taxi_{servicio}_data\"\n        parquet_file = pq.ParquetFile(local_file)\n\n        available_mem = psutil.virtual_memory().available\n        approx_row_mem = 600\n        max_batch_from_mem = int((available_mem * 0.85) / approx_row_mem)\n        batch_size = min(6_000_000, max(100_000, max_batch_from_mem))\n\n        print(f\"Processing {run_id} - RAM: {available_mem / (1024**3):.2f}GB - Batch: {batch_size:,} rows\")\n\n        with loader:\n            for batch_idx, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n                df = batch.to_pandas()\n\n                for k, v in row.items():\n                    if k not in df.columns:\n                        df[k] = v\n\n                hash_input = (\n                    df[\"VendorID\"].astype(str) + \"_\" +\n                    df[\"lpep_pickup_datetime\"].astype(str) + \"_\" +\n                    df[\"lpep_dropoff_datetime\"].astype(str)\n                )\n\n                # OPTIMIZACION: Hash vectorizado (3-4x mas rapido que apply)\n                df[\"row_hash\"] = [hashlib.md5(x.encode()).hexdigest() for x in hash_input.values]\n\n                # OPTIMIZACION: Normalizar columnas una sola vez\n                df.columns = [c.upper() for c in df.columns]\n\n                loader.export(\n                    df,\n                    table_name,\n                    if_exists='append',\n                    unique_conflict_method='update',\n                    unique_constraints=['ROW_HASH'],\n                )\n\n                print(f\"  Batch {batch_idx} exported: {len(df):,} rows\")\n\n                del df, batch, hash_input\n                gc.collect()\n\n        print(f\"Completed {run_id}\")\n        os.remove(local_file)", "file_path": "data_exporters/green_exporter.py", "language": "python", "type": "data_exporter", "uuid": "green_exporter"}, "data_loaders/ingest_taxi.py:data_loader:python:ingest taxi": {"content": "import pandas as pd\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n\n    data = pd.read_csv(url)\n    print(\"Cargando los datos...\")\n    data.columns = [c.strip().upper() for c in data.columns]\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_taxi.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi"}, "data_loaders/taxi_data.py:data_loader:python:taxi data": {"content": "import pandas as pd\nimport requests\nimport uuid\nimport os\nimport pyarrow.parquet as pq\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef check_url(url: str):\n    \"\"\"Verifica si el archivo Parquet existe en la URL y devuelve el response.\"\"\"\n    try:\n        r = requests.head(url, timeout=10)\n        return r.status_code == 200, r\n    except Exception as e:\n        print(f\"[check_url] Error al verificar URL {url}: {e}\")\n        return False, None\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    servicio = kwargs.get(\"service\", \"yellow\")\n    start_year = kwargs.get(\"start_year\", 2025)\n    end_year = kwargs.get(\"end_year\", 2025)\n    start_month = kwargs.get(\"start_month\", 1)\n\n    all_meta = []\n\n    for anio in range(start_year, end_year + 1):\n        # Si es el primer a\u00f1o, empezar desde start_month\n        mes_inicio = start_month if anio == start_year else 1\n        for mes in range(mes_inicio, 13):\n            URL = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{servicio}_tripdata_{anio}-{mes:02d}.parquet\"\n            run_id = f\"{servicio}_{anio}_{mes:02d}\"\n\n            print(f\"Procesando {servicio} {anio}-{mes:02d}\")\n\n            meta = {\n                \"run_id\": run_id,\n                \"anio\": anio,\n                \"mes\": mes,\n                \"servicio\": servicio,\n                \"url\": URL,\n                \"status\": \"pendiente\",\n                \"conteo\": 0,\n                \"n_columns\": 0,\n                \"file_size_bytes\": None,\n                \"ingest_ts\": pd.Timestamp.utcnow(),\n            }\n\n            # verifica disponibilidad\n            ok, response = check_url(URL)\n            if not ok:\n                meta[\"status\"] = \"brecha\"\n                print(f\"Archivo no encontrado en {URL}\")\n                all_meta.append(meta)\n                continue\n\n            # descarga archivo temporal que es el que paso al exporter\n            local_file = f\"/tmp/{servicio}_{anio}_{mes:02d}.parquet\"\n            if not os.path.exists(local_file):\n                print(\"Descargando archivo remoto...\")\n                with requests.get(URL, stream=True, timeout=60) as r:\n                    r.raise_for_status()\n                    with open(local_file, \"wb\") as f:\n                        for chunk in r.iter_content(chunk_size=8192):\n                            f.write(chunk)\n\n            # Leo metadatos con pyarrow\n            parquet_file = pq.ParquetFile(local_file)\n            meta[\"status\"] = \"ok\"\n            meta[\"conteo\"] = parquet_file.metadata.num_rows\n            meta[\"n_columns\"] = len(parquet_file.schema_arrow.names)\n            meta[\"file_size_bytes\"] = os.path.getsize(local_file)\n\n            print(f\"Archivo listo en {local_file}: {meta['conteo']} filas, {meta['n_columns']} columnas\")\n            all_meta.append(meta)\n\n    return pd.DataFrame(all_meta)\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"El loader no devolvi\u00f3 nada\"\n    assert isinstance(output, pd.DataFrame), \"El loader debe devolver un DataFrame \u00fanico\"\n    assert len(output) > 0, \"El DataFrame est\u00e1 vac\u00edo\"\n    print(f\"Test OK: loader devolvi\u00f3 DataFrame con {len(output)} fila(s) de metadatos\")\n", "file_path": "data_loaders/taxi_data.py", "language": "python", "type": "data_loader", "uuid": "taxi_data"}, "data_loaders/green_loader.py:data_loader:python:green loader": {"content": "import pandas as pd\nimport requests\nimport os\nimport pyarrow.parquet as pq\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ndef check_url(url: str):\n    try:\n        r = requests.head(url, timeout=10)\n        return r.status_code == 200, r\n    except Exception as e:\n        print(f\"Error verificando URL {url}: {e}\")\n        return False, None\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    servicio = \"green\"\n    start_year = kwargs.get(\"start_year\", 2015)\n    end_year = kwargs.get(\"end_year\", 2025)\n    start_month = kwargs.get(\"start_month\", 1)\n\n    all_meta = []\n\n    print(f\"Cargando datos GREEN desde {start_year}-{start_month:02d} hasta {end_year}-12\")\n\n    for anio in range(start_year, end_year + 1):\n        mes_inicio = start_month if anio == start_year else 1\n\n        for mes in range(mes_inicio, 13):\n            URL = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{servicio}_tripdata_{anio}-{mes:02d}.parquet\"\n            run_id = f\"{servicio}_{anio}_{mes:02d}\"\n\n            print(f\"Procesando {run_id}\")\n\n            meta = {\n                \"run_id\": run_id,\n                \"anio\": anio,\n                \"mes\": mes,\n                \"servicio\": servicio,\n                \"url\": URL,\n                \"status\": \"pendiente\",\n                \"conteo\": 0,\n                \"n_columns\": 0,\n                \"file_size_bytes\": None,\n                \"ingest_ts\": pd.Timestamp.utcnow(),\n            }\n\n            ok, response = check_url(URL)\n            if not ok:\n                meta[\"status\"] = \"brecha\"\n                print(f\"  No disponible\")\n                all_meta.append(meta)\n                continue\n\n            local_file = f\"/tmp/{servicio}_{anio}_{mes:02d}.parquet\"\n            if not os.path.exists(local_file):\n                print(f\"  Descargando...\")\n                with requests.get(URL, stream=True, timeout=60) as r:\n                    r.raise_for_status()\n                    with open(local_file, \"wb\") as f:\n                        for chunk in r.iter_content(chunk_size=8192):\n                            f.write(chunk)\n\n            parquet_file = pq.ParquetFile(local_file)\n            meta[\"status\"] = \"ok\"\n            meta[\"conteo\"] = parquet_file.metadata.num_rows\n            meta[\"n_columns\"] = len(parquet_file.schema_arrow.names)\n            meta[\"file_size_bytes\"] = os.path.getsize(local_file)\n\n            print(f\"  Listo: {meta['conteo']:,} filas\")\n            all_meta.append(meta)\n\n    return pd.DataFrame(all_meta)\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"El loader no devolvi\u00f3 nada\"\n    assert isinstance(output, pd.DataFrame), \"El loader debe devolver un DataFrame\"\n    assert len(output) > 0, \"El DataFrame est\u00e1 vac\u00edo\"\n    print(f\"Test OK: {len(output)} archivos procesados\")\n", "file_path": "data_loaders/green_loader.py", "language": "python", "type": "data_loader", "uuid": "green_loader"}, "dbts/dbt.yaml:dbt:yaml:dbt": {"content": "", "file_path": "dbts/dbt.yaml", "language": "yaml", "type": "dbt", "uuid": "dbt"}, "pipelines/taxi_zones/__init__.py:pipeline:python:taxi zones/  init  ": {"content": "", "file_path": "pipelines/taxi_zones/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_zones/__init__"}, "pipelines/taxi_zones/metadata.yaml:pipeline:yaml:taxi zones/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - exporter_taxi\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_taxi\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_taxi\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: exporter_taxi\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_taxi\n  uuid: exporter_taxi\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-20 17:36:08.421506+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: taxi_zones\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: taxi_zones\nvariables_dir: /home/src/scheduler\nwidgets: []\n", "file_path": "pipelines/taxi_zones/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_zones/metadata"}, "pipelines/taxi_data/__init__.py:pipeline:python:taxi data/  init  ": {"content": "", "file_path": "pipelines/taxi_data/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_data/__init__"}, "pipelines/taxi_data/metadata.yaml:pipeline:yaml:taxi data/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - taxi_data_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: taxi_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: taxi_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: taxi_data_exporter\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - taxi_data\n  uuid: taxi_data_exporter\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - green_exporter\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: green_loader\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: green_loader\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - dbt_runner\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: green_exporter\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - green_loader\n  uuid: green_exporter\n- all_upstream_blocks_executed: true\n  color: grey\n  configuration: {}\n  downstream_blocks:\n  - dbt_test\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dbt_runner\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - green_exporter\n  uuid: dbt_runner\n- all_upstream_blocks_executed: true\n  color: pink\n  configuration: {}\n  downstream_blocks:\n  - dbt_docs\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dbt_test\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt_runner\n  uuid: dbt_test\n- all_upstream_blocks_executed: true\n  color: teal\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dbt-docs\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt_test\n  uuid: dbt_docs\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-29 03:59:45.748610+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: taxi_data\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: taxi_data\nvariables_dir: /home/src/scheduler\nwidgets: []\n", "file_path": "pipelines/taxi_data/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_data/metadata"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}