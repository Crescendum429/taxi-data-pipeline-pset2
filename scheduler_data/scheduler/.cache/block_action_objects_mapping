{"block_file": {"data_exporters/exporter_taxi.py:data_exporter:python:exporter taxi": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.snowflake import Snowflake\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_snowflake(df: DataFrame, **kwargs) -> None:\n\n    table_name = 'taxi_zones'\n    database = 'NY_TAXI'\n    schema = 'RAW'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Snowflake.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            table_name,\n            database,\n            schema,\n            if_exists='replace',  # Specify resolution policy if table already exists\n        )", "file_path": "data_exporters/exporter_taxi.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxi"}, "data_loaders/ingest_taxi.py:data_loader:python:ingest taxi": {"content": "import pandas as pd\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime\nimport logging\nfrom typing import List, Dict, Tuple\nimport time\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@data_loader\ndef load_nyc_taxi_data(*args, **kwargs):\n    \"\"\"\n    Load NYC Taxi data from Parquet URLs with parallel processing\n    \n    Parameters from kwargs:\n    - year: Target year (2015-2025)\n    - month: Target month (1-12) \n    - service_type: 'yellow' or 'green'\n    - max_workers: Number of parallel threads (default: 4)\n    - chunk_months: Number of months to process in this batch (default: 1)\n    \"\"\"\n\n    # Get parameters\n    year = kwargs.get('year', 2024)\n    month = kwargs.get('month', 1)\n    service_type = kwargs.get('service_type', 'yellow')\n    max_workers = kwargs.get('max_workers', 4)\n    chunk_months = kwargs.get('chunk_months', 1)\n\n    # Generate URL list for the chunk\n    urls_to_process = generate_url_list(year, month, service_type, chunk_months)\n\n    logger.info(f\"\ud83d\udce5 Loading {len(urls_to_process)} files for {service_type} taxi data\")\n    logger.info(f\"   Using {max_workers} parallel workers\")\n    logger.info(f\"   Processing year: {year}, starting month: {month}\")\n\n    # Process files in parallel\n    all_dataframes = []\n    successful_loads = []\n    failed_loads = []\n\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all download tasks\n        future_to_url = {\n            executor.submit(load_parquet_file, url, idx): url\n            for idx, url in enumerate(urls_to_process)\n        }\n\n        # Collect results as they complete\n        for future in as_completed(future_to_url):\n            url = future_to_url[future]\n            try:\n                df, metadata = future.result()\n                if df is not None:\n                    all_dataframes.append(df)\n                    successful_loads.append(metadata)\n                    logger.info(f\"\u2705 Loaded: {metadata['source_file']} ({metadata['rows_loaded']:,} rows)\")\n                else:\n                    failed_loads.append({'url': url, 'error': 'No data returned'})\n            except Exception as e:\n                failed_loads.append({'url': url, 'error': str(e)})\n                logger.error(f\"\u274c Failed to load {url}: {e}\")\n\n    # Combine all dataframes\n    if all_dataframes:\n        logger.info(f\"\ud83d\udd04 Combining {len(all_dataframes)} dataframes...\")\n        combined_df = pd.concat(all_dataframes, ignore_index=True)\n\n        # Add batch metadata\n        combined_df['batch_id'] = f\"{service_type}_{year}_{month:02d}\"\n        combined_df['ingestion_timestamp'] = datetime.now()\n        combined_df['service_type'] = service_type\n\n        # Log summary\n        logger.info(f\"\u2705 Data loading completed!\")\n        logger.info(f\"   Combined dataset shape: {combined_df.shape}\")\n        logger.info(f\"   Successful loads: {len(successful_loads)}\")\n        logger.info(f\"   Failed loads: {len(failed_loads)}\")\n        logger.info(f\"   Total rows: {len(combined_df):,}\")\n\n        if failed_loads:\n            logger.warning(f\"\u26a0\ufe0f  Some files failed to load:\")\n            for failed in failed_loads:\n                logger.warning(f\"   - {failed['url']}: {failed['error']}\")\n\n        return combined_df\n    else:\n        raise Exception(\"\u274c No data was successfully loaded from any source\")\n\ndef generate_url_list(year: int, month: int, service_type: str, chunk_months: int) -> List[str]:\n    \"\"\"Generate list of Parquet URLs to load\"\"\"\n    urls = []\n    base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n\n    for month_offset in range(chunk_months):\n        current_month = month + month_offset\n        current_year = year\n\n        # Handle year rollover\n        if current_month > 12:\n            current_month = current_month - 12\n            current_year = year + 1\n\n        url = f\"{base_url}/{service_type}_tripdata_{current_year}-{current_month:02d}.parquet\"\n        urls.append(url)\n\n    return urls\n\ndef load_parquet_file(url: str, file_index: int) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"Load a single Parquet file from URL\"\"\"\n    try:\n        start_time = time.time()\n        logger.info(f\"\ud83d\udce5 Loading file {file_index + 1}: {url}\")\n\n        # Check if file exists\n        response = requests.head(url, timeout=30)\n        if response.status_code != 200:\n            logger.warning(f\"File not available: {url} (Status: {response.status_code})\")\n            return None, None\n\n        # Load the parquet file directly from URL\n        df = pd.read_parquet(url)\n\n        # Basic data validation\n        if df.empty:\n            logger.warning(f\"Empty dataset from {url}\")\n            return None, None\n\n        # Add source metadata\n        df['source_file'] = url.split('/')[-1]\n        df['file_index'] = file_index\n        df['load_timestamp'] = datetime.now()\n\n        # Processing time\n        processing_time = time.time() - start_time\n\n        metadata = {\n            'source_file': url.split('/')[-1],\n            'file_index': file_index,\n            'rows_loaded': len(df),\n            'columns_loaded': len(df.columns),\n            'processing_time_seconds': round(processing_time, 2),\n            'file_size_mb': round(int(response.headers.get('content-length', 0)) / (1024*1024), 2) if response.headers.get('content-length') else 'unknown'\n        }\n\n        logger.info(f\"\ud83d\udcca File {file_index + 1}: {len(df):,} rows, {len(df.columns)} columns, {processing_time:.2f}s\")\n\n        return df, metadata\n\n    except Exception as e:\n        logger.error(f\"Error loading {url}: {e}\")\n        return None, None\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"Test the loaded data\"\"\"\n    assert output is not None, 'The output is undefined'\n    assert len(output) > 0, 'No data was loaded'\n    assert 'service_type' in output.columns, 'service_type column missing'\n    assert 'batch_id' in output.columns, 'batch_id column missing'\n    assert 'ingestion_timestamp' in output.columns, 'ingestion_timestamp column missing'\n    assert 'source_file' in output.columns, 'source_file column missing'\n\n    # Basic data quality checks\n    required_taxi_columns = ['VendorID', 'pickup_datetime', 'dropoff_datetime'] if'pickup_datetime' in output.columns else ['VendorID']\n    for col in required_taxi_columns:\n        if col in output.columns:\n            assert not output[col].isna().all(), f'Column {col} is all null'\n\n    logger.info(f\"\u2705 Test passed: {len(output):,} rows loaded successfully\")\n    logger.info(f\"   Columns: {list(output.columns)}\")\n    logger.info(f\"   Date range: {output['load_timestamp'].min()} to {output['load_timestamp'].max()}\")", "file_path": "data_loaders/ingest_taxi.py", "language": "python", "type": "data_loader", "uuid": "ingest_taxi"}, "pipelines/taxi_zones/__init__.py:pipeline:python:taxi zones/  init  ": {"content": "", "file_path": "pipelines/taxi_zones/__init__.py", "language": "python", "type": "pipeline", "uuid": "taxi_zones/__init__"}, "pipelines/taxi_zones/metadata.yaml:pipeline:yaml:taxi zones/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - exporter_taxi\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_taxi\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_taxi\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: exporter_taxi\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_taxi\n  uuid: exporter_taxi\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-20 17:36:08.421506+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: taxi_zones\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: taxi_zones\nvariables_dir: /home/src/scheduler\nwidgets: []\n", "file_path": "pipelines/taxi_zones/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "taxi_zones/metadata"}, "/home/src/scheduler/data_exporters/exporter_taxi.py:data_exporter:python:home/src/scheduler/data exporters/exporter taxi": {"content": "import pandas as pd\nimport snowflake.connector\nfrom datetime import datetime\nimport logging\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nlogger = logging.getLogger(__name__)\n\n@data_exporter\ndef export_to_snowflake_bronze(df: pd.DataFrame, *args, **kwargs) -> None:\n    from mage_ai.data_preparation.shared.secrets import get_secret_value\n\n    connection_params = {\n        'account': get_secret_value('SNOWFLAKE_ACCOUNT'),\n        'user': get_secret_value('SNOWFLAKE_USER'),\n        'password': get_secret_value('SNOWFLAKE_PASSWORD'),\n        'database': get_secret_value('SNOWFLAKE_DATABASE'),\n        'warehouse': get_secret_value('SNOWFLAKE_WAREHOUSE'),\n        'role': get_secret_value('SNOWFLAKE_ROLE'),\n        'schema': kwargs.get('schema_name', 'RAW')\n    }\n\n    missing_params = [key for key, value in connection_params.items() if not value]\n    if missing_params:\n        raise ValueError(f\"Missing Snowflake connection parameters: {missing_params}\")\n\n    service_type = df['service_type'].iloc[0] if 'service_type' in df.columns else kwargs.get('service_type', 'unknown')\n    table_name = kwargs.get('table_name', f\"{service_type.upper()}_TRIPS\")\n\n    logger.info(f\"Exporting to {connection_params['database']}.{connection_params['schema']}.{table_name}\")\n    logger.info(f\"Rows: {len(df)}, Service: {service_type}\")\n\n    conn = snowflake.connector.connect(**connection_params)\n    cursor = conn.cursor()\n\n    try:\n        create_bronze_table_if_not_exists(cursor, table_name, service_type)\n\n        df_export = prepare_dataframe_for_snowflake(df)\n\n        rows_inserted = insert_dataframe_batch(cursor, table_name, df_export)\n\n        logger.info(f\"Export completed: {rows_inserted} rows inserted\")\n\n    finally:\n        cursor.close()\n        conn.close()\n\ndef prepare_dataframe_for_snowflake(df: pd.DataFrame) -> pd.DataFrame:\n    df_clean = df.copy()\n\n    datetime_columns = [col for col in df_clean.columns if 'datetime' in col.lower() or 'timestamp' in col.lower()]\n    for col in datetime_columns:\n        if df_clean[col].dtype == 'object':\n            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n\n    numeric_columns = [col for col in df_clean.columns if any(x in col.lower() for x in ['amount', 'fare', 'tip', 'distance'])]\n    for col in numeric_columns:\n        if col in df_clean.columns:\n            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n\n    df_clean['SNOWFLAKE_LOADED_AT'] = datetime.now()\n    df_clean.columns = [col.upper() for col in df_clean.columns]\n\n    return df_clean\n\ndef create_bronze_table_if_not_exists(cursor, table_name: str, service_type: str):\n    cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n    if cursor.fetchone():\n        return\n\n    create_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name} (\n        VENDORID NUMBER,\n        TPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n        TPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n        PASSENGER_COUNT NUMBER,\n        TRIP_DISTANCE FLOAT,\n        RATECODEID NUMBER,\n        STORE_AND_FWD_FLAG STRING,\n        PULOCATIONID NUMBER,\n        DOLOCATIONID NUMBER,\n        PAYMENT_TYPE NUMBER,\n        FARE_AMOUNT FLOAT,\n        EXTRA FLOAT,\n        MTA_TAX FLOAT,\n        TIP_AMOUNT FLOAT,\n        TOLLS_AMOUNT FLOAT,\n        IMPROVEMENT_SURCHARGE FLOAT,\n        TOTAL_AMOUNT FLOAT,\n        CONGESTION_SURCHARGE FLOAT,\n        AIRPORT_FEE FLOAT,\n        SOURCE_FILE STRING,\n        FILE_INDEX NUMBER,\n        LOAD_TIMESTAMP TIMESTAMP_NTZ,\n        BATCH_ID STRING,\n        INGESTION_TIMESTAMP TIMESTAMP_NTZ,\n        SERVICE_TYPE STRING,\n        SNOWFLAKE_LOADED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n    )\n    \"\"\"\n\n    cursor.execute(create_sql)\n    logger.info(f\"Table {table_name} created\")\n\ndef insert_dataframe_batch(cursor, table_name: str, df: pd.DataFrame, batch_size: int = 1000) -> int:\n    columns = list(df.columns)\n    placeholders = ', '.join(['%s'] * len(columns))\n    insert_sql = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES ({placeholders})\"\n\n    total_inserted = 0\n\n    for i in range(0, len(df), batch_size):\n        batch_df = df.iloc[i:i + batch_size]\n\n        batch_data = []\n        for _, row in batch_df.iterrows():\n            row_data = []\n            for val in row:\n                if pd.isna(val):\n                    row_data.append(None)\n                elif isinstance(val, pd.Timestamp):\n                    row_data.append(val.to_pydatetime())\n                else:\n                    row_data.append(val)\n            batch_data.append(tuple(row_data))\n\n        cursor.executemany(insert_sql, batch_data)\n        total_inserted += len(batch_data)\n\n        if i % 10000 == 0:\n            logger.info(f\"Inserted {total_inserted} rows so far...\")\n\n    return total_inserted\n", "file_path": "/home/src/scheduler/data_exporters/exporter_taxi.py", "language": "python", "type": "data_exporter", "uuid": "exporter_taxi"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}