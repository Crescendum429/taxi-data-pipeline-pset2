import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import logging
from typing import List, Dict, Tuple, Optional
import time
import numpy as np

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test

logger = logging.getLogger(__name__)

@data_loader
def load_nyc_taxi_jan_2015(*args, **kwargs):
    year = 2015
    month = 1
    service_type = 'yellow'
    max_workers = 8
    validate_data = True

    urls_to_process = generate_url_list(year, month, service_type, 1)

    logger.info(f"Loading January 2015 Yellow Taxi data")
    logger.info(f"URLs to process: {urls_to_process}")
    logger.info(f"Using {max_workers} parallel workers")

    start_time = time.time()
    all_dataframes = []
    successful_loads = []
    failed_loads = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(load_parquet_optimized, url, idx): url
            for idx, url in enumerate(urls_to_process)
        }

        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                df, metadata = future.result()
                if df is not None:
                    all_dataframes.append(df)
                    successful_loads.append(metadata)
                    logger.info(f"Loaded: {metadata['source_file']} ({metadata['rows_loaded']:,} rows)")
                else:
                    failed_loads.append({'url': url, 'error': 'No data returned'})
            except Exception as e:
                failed_loads.append({'url': url, 'error': str(e)})
                logger.error(f"Failed to load {url}: {e}")

    if not all_dataframes:
        raise Exception("No data was successfully loaded from any source")

    logger.info(f"Combining {len(all_dataframes)} dataframes...")
    combined_df = pd.concat(all_dataframes, ignore_index=True)

    combined_df['batch_id'] = f"{service_type}_{year}_{month:02d}"
    combined_df['ingestion_timestamp'] = datetime.now()
    combined_df['service_type'] = service_type

    if validate_data:
        combined_df = apply_data_quality_rules(combined_df)

    total_time = time.time() - start_time
    rows_per_second = len(combined_df) / total_time if total_time > 0 else 0

    logger.info(f"January 2015 data loading completed!")
    logger.info(f"Total rows: {len(combined_df):,}")
    logger.info(f"Total time: {total_time:.2f}s ({rows_per_second:.0f} rows/sec)")
    logger.info(f"Successful loads: {len(successful_loads)}")
    logger.info(f"Failed loads: {len(failed_loads)}")

    return combined_df

def generate_url_list(year: int, month: int, service_type: str, chunk_months: int) -> List[str]:
    urls = []
    base_url = "https://d37ci6vzurychx.cloudfront.net/trip-data"

    for month_offset in range(chunk_months):
        current_month = month + month_offset
        current_year = year

        if current_month > 12:
            current_month = current_month - 12
            current_year = year + 1

        url = f"{base_url}/{service_type}_tripdata_{current_year}-{current_month:02d}.parquet"
        urls.append(url)

    return urls

def load_parquet_optimized(url: str, file_index: int) -> Tuple[Optional[pd.DataFrame], Optional[Dict]]:
    try:
        start_time = time.time()

        logger.info(f"Attempting to load: {url}")
        response = requests.head(url, timeout=15)
        if response.status_code != 200:
            logger.warning(f"File not available: {url} (Status: {response.status_code})")
            return None, None

        df = pd.read_parquet(url, engine='pyarrow')

        if df.empty:
            logger.warning(f"Empty dataset from {url}")
            return None, None

        df = optimize_datatypes(df)

        df['source_file'] = url.split('/')[-1]
        df['file_index'] = file_index
        df['load_timestamp'] = datetime.now()

        processing_time = time.time() - start_time
        file_size_mb = int(response.headers.get('content-length', 0)) / (1024*1024) if response.headers.get('content-length') else 0

        metadata = {
            'source_file': url.split('/')[-1],
            'file_index': file_index,
            'rows_loaded': len(df),
            'columns_loaded': len(df.columns),
            'processing_time_seconds': round(processing_time, 2),
            'file_size_mb': round(file_size_mb, 2),
            'load_rate_mbps': round(file_size_mb / processing_time, 2) if processing_time > 0 else 0
        }

        return df, metadata

    except Exception as e:
        logger.error(f"Error loading {url}: {e}")
        return None, None

def optimize_datatypes(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                if 'datetime' in col.lower():
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                elif col.lower() in ['store_and_fwd_flag']:
                    df[col] = df[col].astype('category')
            except:
                pass
        elif df[col].dtype in ['int64', 'float64']:
            if df[col].isnull().sum() == 0:
                if df[col].dtype == 'int64':
                    if df[col].min() >= 0 and df[col].max() <= 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].min() >= -32768 and df[col].max() <= 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
                        df[col] = df[col].astype('int32')
                elif df[col].dtype == 'float64':
                    df[col] = pd.to_numeric(df[col], downcast='float')

    return df

def apply_data_quality_rules(df: pd.DataFrame) -> pd.DataFrame:
    initial_rows = len(df)

    datetime_cols = [col for col in df.columns if 'datetime' in col.lower()]
    for col in datetime_cols:
        if col in df.columns:
            df = df[df[col].notna()]

    numeric_cols = [col for col in df.columns if any(x in col.lower() for x in ['amount', 'fare', 'distance'])]
    for col in numeric_cols:
        if col in df.columns:
            df = df[df[col] >= 0]

    if 'passenger_count' in df.columns:
        df = df[(df['passenger_count'] >= 0) & (df['passenger_count'] <= 10)]

    if 'trip_distance' in df.columns:
        df = df[(df['trip_distance'] >= 0) & (df['trip_distance'] <= 1000)]

    final_rows = len(df)
    filtered_rows = initial_rows - final_rows

    if filtered_rows > 0:
        logger.info(f"Data quality filtering: removed {filtered_rows:,} rows ({(filtered_rows/initial_rows)*100:.1f}%)")

    return df

@test
def test_output(output, *args) -> None:
    assert output is not None, 'The output is undefined'
    assert len(output) > 0, 'No data was loaded'
    assert 'service_type' in output.columns, 'service_type column missing'
    assert 'batch_id' in output.columns, 'batch_id column missing'
    assert 'ingestion_timestamp' in output.columns, 'ingestion_timestamp column missing'
    assert 'source_file' in output.columns, 'source_file column missing'

    required_taxi_columns = ['VendorID'] if 'VendorID' in output.columns else []
    for col in required_taxi_columns:
        assert not output[col].isna().all(), f'Column {col} is all null'

    logger.info(f"Test passed: {len(output):,} rows loaded successfully")
    logger.info(f"Memory usage: {output.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
