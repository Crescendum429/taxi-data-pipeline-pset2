import pandas as pd
import requests
from datetime import datetime
import logging
import time
import numpy as np
from typing import Optional, Dict

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test

logger = logging.getLogger(__name__)

@data_loader
def load_yellow_taxi_2015_01(*args, **kwargs) -> pd.DataFrame:
    year = 2015
    month = 1
    service_type = 'yellow'

    url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{service_type}_tripdata_{year}-{month:02d}.parquet"

    start_time = time.time()

    logger.info(f"Loading Yellow Taxi data for {year}-{month:02d}")
    logger.info(f"Source URL: {url}")

    try:
        response = requests.head(url, timeout=30)
        if response.status_code != 200:
            raise Exception(f"File not available: {url} (Status: {response.status_code})")

        file_size_mb = int(response.headers.get('content-length', 0)) / (1024*1024)
        logger.info(f"File size: {file_size_mb:.1f} MB")

        df = pd.read_parquet(url, engine='pyarrow')

        if df.empty:
            raise Exception(f"Empty dataset from {url}")

        df = optimize_datatypes(df)

        df['source_file'] = f"{service_type}_tripdata_{year}-{month:02d}.parquet"
        df['file_index'] = 0
        df['load_timestamp'] = datetime.now()
        df['batch_id'] = f"{service_type}_{year}_{month:02d}"
        df['ingestion_timestamp'] = datetime.now()
        df['service_type'] = service_type

        df = apply_data_quality_rules(df)

        processing_time = time.time() - start_time
        rows_per_second = len(df) / processing_time if processing_time > 0 else 0
        memory_mb = df.memory_usage(deep=True).sum() / 1024**2

        logger.info(f"Loading completed successfully")
        logger.info(f"Rows loaded: {len(df):,}")
        logger.info(f"Columns: {len(df.columns)}")
        logger.info(f"Processing time: {processing_time:.2f}s")
        logger.info(f"Load rate: {rows_per_second:.0f} rows/sec")
        logger.info(f"Memory usage: {memory_mb:.1f} MB")

        return df

    except Exception as e:
        logger.error(f"Failed to load {url}: {e}")
        raise

def optimize_datatypes(df: pd.DataFrame) -> pd.DataFrame:
    logger.info("Optimizing data types")

    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                if 'datetime' in col.lower():
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                elif col.lower() in ['store_and_fwd_flag']:
                    df[col] = df[col].astype('category')
            except:
                pass
        elif df[col].dtype in ['int64', 'float64']:
            if df[col].isnull().sum() == 0:
                if df[col].dtype == 'int64':
                    if df[col].min() >= 0 and df[col].max() <= 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].min() >= -32768 and df[col].max() <= 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
                        df[col] = df[col].astype('int32')
                elif df[col].dtype == 'float64':
                    df[col] = pd.to_numeric(df[col], downcast='float')

    return df

def apply_data_quality_rules(df: pd.DataFrame) -> pd.DataFrame:
    logger.info("Applying data quality rules")
    initial_rows = len(df)

    datetime_cols = [col for col in df.columns if 'datetime' in col.lower()]
    for col in datetime_cols:
        if col in df.columns:
            df = df[df[col].notna()]

    numeric_cols = [col for col in df.columns if any(x in col.lower() for x in ['amount', 'fare', 'distance'])]
    for col in numeric_cols:
        if col in df.columns:
            df = df[df[col] >= 0]

    if 'passenger_count' in df.columns:
        df = df[(df['passenger_count'] >= 0) & (df['passenger_count'] <= 10)]

    if 'trip_distance' in df.columns:
        df = df[(df['trip_distance'] >= 0) & (df['trip_distance'] <= 1000)]

    final_rows = len(df)
    filtered_rows = initial_rows - final_rows

    if filtered_rows > 0:
        logger.info(f"Data quality filtering: removed {filtered_rows:,} rows ({(filtered_rows/initial_rows)*100:.1f}%)")

    return df

@test
def test_output(output, *args) -> None:
    assert output is not None, 'Output is undefined'
    assert len(output) > 0, 'No data was loaded'
    assert 'service_type' in output.columns, 'service_type column missing'
    assert 'batch_id' in output.columns, 'batch_id column missing'
    assert 'ingestion_timestamp' in output.columns, 'ingestion_timestamp column missing'
    assert 'source_file' in output.columns, 'source_file column missing'

    required_columns = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']
    for col in required_columns:
        if col in output.columns:
            assert not output[col].isna().all(), f'Column {col} is all null'

    logger.info(f"Test passed: {len(output):,} rows loaded successfully")
    logger.info(f"Service type: {output['service_type'].iloc[0]}")
    logger.info(f"Date range: {output['tpep_pickup_datetime'].min()} to {output['tpep_pickup_datetime'].max()}")
